#!/usr/bin/env python3
"""
ğŸ§  Dawn of Stellar - í†µí•© ì–¸ì–´ëª¨ë¸ ë§¤ë‹ˆì €
ëª¨ë“  ë¡œì»¬/ì›ê²© ì–¸ì–´ëª¨ë¸ì„ í†µí•© ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ

2025ë…„ 8ì›” 14ì¼ - 3ê°€ì§€ LLM ì‹œìŠ¤í…œ í†µí•©
"""

import json
import requests
import time
import asyncio
import aiohttp
from enum import Enum
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass
import os
from pathlib import Path

# ìƒ‰ìƒ ì •ì˜
RESET = '\033[0m'
BOLD = '\033[1m'
GREEN = '\033[92m'
YELLOW = '\033[93m'
BLUE = '\033[94m'
RED = '\033[91m'
CYAN = '\033[96m'

class LLMType(Enum):
    """ì–¸ì–´ëª¨ë¸ íƒ€ì…"""
    OLLAMA = "ollama"           # ë¡œì»¬ Ollama
    EXAONE = "exaone"          # EXAONE 3.5
    OPENAI = "openai"          # OpenAI GPT
    CLAUDE = "claude"          # Anthropic Claude
    GEMINI = "gemini"          # Google Gemini
    AUTO = "auto"              # ìë™ ì„ íƒ

@dataclass
class LLMConfig:
    """ì–¸ì–´ëª¨ë¸ ì„¤ì •"""
    name: str
    type: LLMType
    endpoint: str
    model_name: str
    api_key: Optional[str] = None
    available: bool = False
    priority: int = 0  # ë‚®ì„ìˆ˜ë¡ ë†’ì€ ìš°ì„ ìˆœìœ„

class UnifiedLanguageModelManager:
    """í†µí•© ì–¸ì–´ëª¨ë¸ ë§¤ë‹ˆì €"""
    
    def __init__(self):
        self.configs = {}
        self.current_llm = None
        self.conversation_history = []
        self.max_history = 10
        
        # ì„¤ì • íŒŒì¼ ê²½ë¡œ
        self.config_file = Path("game_settings.json")
        
        # ê¸°ë³¸ LLM ì„¤ì •ë“¤
        self._initialize_llm_configs()
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ LLM ê²€ì‚¬
        self._check_all_llm_availability()
        
        # ìµœì  LLM ì„ íƒ
        self._select_best_llm()
        
        print(f"ğŸ§  í†µí•© ì–¸ì–´ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ!")
        print(f"   í˜„ì¬ LLM: {self.current_llm.name if self.current_llm else 'ì—†ìŒ'}")
    
    def _initialize_llm_configs(self):
        """LLM ì„¤ì • ì´ˆê¸°í™”"""
        
        # 1. Ollama (ìµœìš°ì„  - ë¡œì»¬, ë¬´ë£Œ)
        self.configs[LLMType.OLLAMA] = LLMConfig(
            name="Ollama",
            type=LLMType.OLLAMA,
            endpoint="http://localhost:11434",
            model_name="llama3.1:8b",
            priority=1
        )
        
        # 2. EXAONE (í•œêµ­ì–´ íŠ¹í™”)
        self.configs[LLMType.EXAONE] = LLMConfig(
            name="EXAONE 3.5",
            type=LLMType.EXAONE,
            endpoint="http://localhost:11434",
            model_name="exaone3.5:7.8b",
            priority=2
        )
        
        # 3. OpenAI (ìœ ë£Œ, ê³ í’ˆì§ˆ)
        self.configs[LLMType.OPENAI] = LLMConfig(
            name="OpenAI GPT",
            type=LLMType.OPENAI,
            endpoint="https://api.openai.com/v1",
            model_name="gpt-3.5-turbo",
            api_key=os.getenv("OPENAI_API_KEY"),
            priority=3
        )
        
        # 4. Claude (ìœ ë£Œ, ê³ í’ˆì§ˆ)
        self.configs[LLMType.CLAUDE] = LLMConfig(
            name="Claude",
            type=LLMType.CLAUDE,
            endpoint="https://api.anthropic.com",
            model_name="claude-3-haiku-20240307",
            api_key=os.getenv("ANTHROPIC_API_KEY"),
            priority=4
        )
    
    def _check_ollama_availability(self) -> bool:
        """Ollama ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸"""
        try:
            config = self.configs[LLMType.OLLAMA]
            response = requests.get(f"{config.endpoint}/api/tags", timeout=3)
            
            if response.status_code == 200:
                models = response.json().get('models', [])
                available_models = [model['name'] for model in models]
                
                # llama ê³„ì—´ ëª¨ë¸ í™•ì¸
                for model_name in available_models:
                    if 'llama' in model_name.lower():
                        config.model_name = model_name
                        return True
                        
        except Exception as e:
            print(f"âš ï¸ Ollama ì—°ê²° ì‹¤íŒ¨: {e}")
        
        return False
    
    def _check_exaone_availability(self) -> bool:
        """EXAONE ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸"""
        try:
            config = self.configs[LLMType.EXAONE]
            response = requests.get(f"{config.endpoint}/api/tags", timeout=3)
            
            if response.status_code == 200:
                models = response.json().get('models', [])
                available_models = [model['name'] for model in models]
                
                # EXAONE ëª¨ë¸ í™•ì¸
                for model_name in available_models:
                    if 'exaone' in model_name.lower():
                        config.model_name = model_name
                        return True
                        
        except Exception as e:
            print(f"âš ï¸ EXAONE ì—°ê²° ì‹¤íŒ¨: {e}")
        
        return False
    
    def _check_openai_availability(self) -> bool:
        """OpenAI ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸"""
        try:
            config = self.configs[LLMType.OPENAI]
            if not config.api_key:
                return False
                
            headers = {"Authorization": f"Bearer {config.api_key}"}
            response = requests.get(f"{config.endpoint}/models", 
                                  headers=headers, timeout=5)
            
            return response.status_code == 200
            
        except Exception as e:
            print(f"âš ï¸ OpenAI ì—°ê²° ì‹¤íŒ¨: {e}")
        
        return False
    
    def _check_claude_availability(self) -> bool:
        """Claude ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸"""
        try:
            config = self.configs[LLMType.CLAUDE]
            if not config.api_key:
                return False
                
            # Claude APIëŠ” ê°„ë‹¨í•œ ìƒíƒœ í™•ì¸ì´ ì–´ë ¤ìš°ë¯€ë¡œ API í‚¤ ì¡´ì¬ ì—¬ë¶€ë§Œ í™•ì¸
            return True
            
        except Exception as e:
            print(f"âš ï¸ Claude ì—°ê²° ì‹¤íŒ¨: {e}")
        
        return False
    
    def _check_all_llm_availability(self):
        """ëª¨ë“  LLM ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸"""
        print("ğŸ” ì–¸ì–´ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸ ì¤‘...")
        
        # Ollama í™•ì¸
        self.configs[LLMType.OLLAMA].available = self._check_ollama_availability()
        print(f"   Ollama: {'âœ…' if self.configs[LLMType.OLLAMA].available else 'âŒ'}")
        
        # EXAONE í™•ì¸
        self.configs[LLMType.EXAONE].available = self._check_exaone_availability()
        print(f"   EXAONE: {'âœ…' if self.configs[LLMType.EXAONE].available else 'âŒ'}")
        
        # OpenAI í™•ì¸
        self.configs[LLMType.OPENAI].available = self._check_openai_availability()
        print(f"   OpenAI: {'âœ…' if self.configs[LLMType.OPENAI].available else 'âŒ'}")
        
        # Claude í™•ì¸
        self.configs[LLMType.CLAUDE].available = self._check_claude_availability()
        print(f"   Claude: {'âœ…' if self.configs[LLMType.CLAUDE].available else 'âŒ'}")
    
    def _select_best_llm(self):
        """ìµœì  LLM ì„ íƒ"""
        available_llms = [
            config for config in self.configs.values() 
            if config.available
        ]
        
        if available_llms:
            # ìš°ì„ ìˆœìœ„ìˆœìœ¼ë¡œ ì •ë ¬
            available_llms.sort(key=lambda x: x.priority)
            self.current_llm = available_llms[0]
            print(f"ğŸ¯ ì„ íƒëœ LLM: {self.current_llm.name}")
        else:
            self.current_llm = None
            print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì–¸ì–´ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
    
    def get_available_llms(self) -> List[LLMConfig]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª©ë¡ ë°˜í™˜"""
        return [config for config in self.configs.values() if config.available]
    
    def switch_llm(self, llm_type: LLMType) -> bool:
        """LLM ë³€ê²½"""
        if llm_type in self.configs and self.configs[llm_type].available:
            self.current_llm = self.configs[llm_type]
            print(f"ğŸ”„ LLM ë³€ê²½: {self.current_llm.name}")
            return True
        else:
            print(f"âŒ {llm_type.value} LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
    
    def _generate_system_prompt(self, character_name: str, character_class: str, 
                              personality: str, situation: str) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        return f"""ë‹¹ì‹ ì€ Dawn of Stellar ê²Œì„ì˜ AI ë™ë£Œ '{character_name}'ì…ë‹ˆë‹¤.

ğŸ“‹ ìºë¦­í„° ì •ë³´:
- ì´ë¦„: {character_name}
- ì§ì—…: {character_class}
- ì„±ê²©: {personality}
- í˜„ì¬ ìƒí™©: {situation}

ğŸ¯ ì—­í• :
- ê²Œì„ í”Œë ˆì´ì–´ì˜ ë™ë£Œë¡œì„œ í•¨ê»˜ ëª¨í—˜ì„ ë– ë‚©ë‹ˆë‹¤
- ì „íˆ¬ì—ì„œ ë„ì›€ì„ ì£¼ê³  ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤
- í”Œë ˆì´ì–´ì™€ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ë©° ê²Œì„ì„ ì¦ê²ê²Œ ë§Œë“­ë‹ˆë‹¤

ğŸ’¬ ëŒ€í™” ê·œì¹™:
- í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”
- ìºë¦­í„°ì˜ ì„±ê²©ê³¼ ì§ì—…ì— ë§ê²Œ ë§í•˜ì„¸ìš”
- ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ 2-3ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
- ê²Œì„ ìƒí™©ì— ë§ëŠ” ì ì ˆí•œ ë°˜ì‘ì„ ë³´ì´ì„¸ìš”
- ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•´ì„œ ìƒë™ê° ìˆê²Œ í‘œí˜„í•˜ì„¸ìš”"""
    
    async def generate_response_async(self, user_message: str, character_name: str = "AIë™ë£Œ",
                                    character_class: str = "ì „ì‚¬", personality: str = "ì¹œê·¼í•œ",
                                    situation: str = "ë˜ì „ íƒí—˜") -> str:
        """ë¹„ë™ê¸° ì‘ë‹µ ìƒì„±"""
        
        if not self.current_llm:
            return "ì£„ì†¡í•©ë‹ˆë‹¤, í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ì–¸ì–´ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ğŸ˜…"
        
        try:
            if self.current_llm.type in [LLMType.OLLAMA, LLMType.EXAONE]:
                return await self._generate_ollama_response_async(
                    user_message, character_name, character_class, personality, situation
                )
            elif self.current_llm.type == LLMType.OPENAI:
                return await self._generate_openai_response_async(
                    user_message, character_name, character_class, personality, situation
                )
            elif self.current_llm.type == LLMType.CLAUDE:
                return await self._generate_claude_response_async(
                    user_message, character_name, character_class, personality, situation
                )
            else:
                return "ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–¸ì–´ëª¨ë¸ì…ë‹ˆë‹¤. ğŸ¤”"
                
        except Exception as e:
            print(f"âš ï¸ {self.current_llm.name} ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # í´ë°±: ë‹¤ë¥¸ LLM ì‹œë„
            return await self._try_fallback_llm(user_message, character_name, 
                                              character_class, personality, situation)
    
    def generate_response(self, user_message: str, character_name: str = "AIë™ë£Œ",
                         character_class: str = "ì „ì‚¬", personality: str = "ì¹œê·¼í•œ",
                         situation: str = "ë˜ì „ íƒí—˜") -> str:
        """ë™ê¸° ì‘ë‹µ ìƒì„± (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±)"""
        
        try:
            # ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸°ë¡œ ì‹¤í–‰
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            result = loop.run_until_complete(
                self.generate_response_async(user_message, character_name, 
                                           character_class, personality, situation)
            )
            loop.close()
            return result
        except Exception as e:
            print(f"âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ìŒ... ë­”ê°€ ë¬¸ì œê°€ ìˆëŠ” ê²ƒ ê°™ì•„ìš”. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”? ğŸ˜…"
    
    async def _generate_ollama_response_async(self, user_message: str, character_name: str,
                                            character_class: str, personality: str, situation: str) -> str:
        """Ollama/EXAONE ì‘ë‹µ ìƒì„±"""
        
        system_prompt = self._generate_system_prompt(character_name, character_class, personality, situation)
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        messages = [{"role": "system", "content": system_prompt}]
        
        # ìµœê·¼ ëŒ€í™” ì¶”ê°€
        for msg in self.conversation_history[-self.max_history:]:
            messages.append(msg)
        
        # í˜„ì¬ ë©”ì‹œì§€ ì¶”ê°€
        messages.append({"role": "user", "content": user_message})
        
        try:
            async with aiohttp.ClientSession() as session:
                data = {
                    "model": self.current_llm.model_name,
                    "messages": messages,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "max_tokens": 200
                    }
                }
                
                async with session.post(f"{self.current_llm.endpoint}/api/chat", 
                                      json=data, timeout=30) as response:
                    if response.status == 200:
                        result = await response.json()
                        ai_response = result.get('message', {}).get('content', '')
                        
                        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
                        self.conversation_history.append({"role": "user", "content": user_message})
                        self.conversation_history.append({"role": "assistant", "content": ai_response})
                        
                        return ai_response
                    else:
                        raise Exception(f"HTTP {response.status}")
                        
        except Exception as e:
            raise Exception(f"Ollama ìš”ì²­ ì‹¤íŒ¨: {e}")
    
    async def _generate_openai_response_async(self, user_message: str, character_name: str,
                                            character_class: str, personality: str, situation: str) -> str:
        """OpenAI ì‘ë‹µ ìƒì„±"""
        
        system_prompt = self._generate_system_prompt(character_name, character_class, personality, situation)
        
        messages = [{"role": "system", "content": system_prompt}]
        
        # ìµœê·¼ ëŒ€í™” ì¶”ê°€
        for msg in self.conversation_history[-self.max_history:]:
            messages.append(msg)
        
        messages.append({"role": "user", "content": user_message})
        
        try:
            async with aiohttp.ClientSession() as session:
                headers = {
                    "Authorization": f"Bearer {self.current_llm.api_key}",
                    "Content-Type": "application/json"
                }
                
                data = {
                    "model": self.current_llm.model_name,
                    "messages": messages,
                    "max_tokens": 200,
                    "temperature": 0.7
                }
                
                async with session.post(f"{self.current_llm.endpoint}/chat/completions",
                                      headers=headers, json=data, timeout=30) as response:
                    if response.status == 200:
                        result = await response.json()
                        ai_response = result['choices'][0]['message']['content']
                        
                        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
                        self.conversation_history.append({"role": "user", "content": user_message})
                        self.conversation_history.append({"role": "assistant", "content": ai_response})
                        
                        return ai_response
                    else:
                        raise Exception(f"HTTP {response.status}")
                        
        except Exception as e:
            raise Exception(f"OpenAI ìš”ì²­ ì‹¤íŒ¨: {e}")
    
    async def _generate_claude_response_async(self, user_message: str, character_name: str,
                                            character_class: str, personality: str, situation: str) -> str:
        """Claude ì‘ë‹µ ìƒì„±"""
        
        system_prompt = self._generate_system_prompt(character_name, character_class, personality, situation)
        
        try:
            async with aiohttp.ClientSession() as session:
                headers = {
                    "x-api-key": self.current_llm.api_key,
                    "Content-Type": "application/json",
                    "anthropic-version": "2023-06-01"
                }
                
                data = {
                    "model": self.current_llm.model_name,
                    "max_tokens": 200,
                    "system": system_prompt,
                    "messages": [{"role": "user", "content": user_message}]
                }
                
                async with session.post(f"{self.current_llm.endpoint}/v1/messages",
                                      headers=headers, json=data, timeout=30) as response:
                    if response.status == 200:
                        result = await response.json()
                        ai_response = result['content'][0]['text']
                        
                        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
                        self.conversation_history.append({"role": "user", "content": user_message})
                        self.conversation_history.append({"role": "assistant", "content": ai_response})
                        
                        return ai_response
                    else:
                        raise Exception(f"HTTP {response.status}")
                        
        except Exception as e:
            raise Exception(f"Claude ìš”ì²­ ì‹¤íŒ¨: {e}")
    
    async def _try_fallback_llm(self, user_message: str, character_name: str,
                               character_class: str, personality: str, situation: str) -> str:
        """í´ë°± LLM ì‹œë„"""
        
        available_llms = self.get_available_llms()
        
        for llm_config in available_llms:
            if llm_config.type != self.current_llm.type:
                try:
                    print(f"ğŸ”„ í´ë°± LLM ì‹œë„: {llm_config.name}")
                    old_llm = self.current_llm
                    self.current_llm = llm_config
                    
                    response = await self.generate_response_async(
                        user_message, character_name, character_class, personality, situation
                    )
                    
                    return response
                    
                except Exception as e:
                    print(f"âš ï¸ í´ë°± {llm_config.name} ì‹¤íŒ¨: {e}")
                    self.current_llm = old_llm
                    continue
        
        # ëª¨ë“  LLM ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ
        return f"ì£„ì†¡í•´ìš”, ì§€ê¸ˆ ë§ì„ ì˜ ëª» ì•Œì•„ë“£ê² ì–´ìš”. ë‚˜ì¤‘ì— ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”? ğŸ˜…"
    
    def clear_conversation_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_history.clear()
        print("ğŸ—‘ï¸ ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def get_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ìƒíƒœ ë°˜í™˜"""
        return {
            "current_llm": self.current_llm.name if self.current_llm else None,
            "available_llms": [config.name for config in self.get_available_llms()],
            "conversation_length": len(self.conversation_history),
            "all_configs": {
                llm_type.value: {
                    "name": config.name,
                    "available": config.available,
                    "priority": config.priority
                }
                for llm_type, config in self.configs.items()
            }
        }

# ì „ì—­ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
_unified_llm_manager = None

def get_unified_llm_manager() -> UnifiedLanguageModelManager:
    """í†µí•© ì–¸ì–´ëª¨ë¸ ë§¤ë‹ˆì € ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _unified_llm_manager
    if _unified_llm_manager is None:
        _unified_llm_manager = UnifiedLanguageModelManager()
    return _unified_llm_manager

# í¸ì˜ í•¨ìˆ˜ë“¤
def generate_ai_response(user_message: str, character_name: str = "AIë™ë£Œ",
                        character_class: str = "ì „ì‚¬", personality: str = "ì¹œê·¼í•œ",
                        situation: str = "ë˜ì „ íƒí—˜") -> str:
    """AI ì‘ë‹µ ìƒì„± (ê°„ë‹¨í•œ ì¸í„°í˜ì´ìŠ¤)"""
    manager = get_unified_llm_manager()
    return manager.generate_response(user_message, character_name, character_class, personality, situation)

def switch_ai_model(llm_type: LLMType) -> bool:
    """AI ëª¨ë¸ ë³€ê²½"""
    manager = get_unified_llm_manager()
    return manager.switch_llm(llm_type)

def get_ai_status() -> Dict[str, Any]:
    """AI ìƒíƒœ ì¡°íšŒ"""
    manager = get_unified_llm_manager()
    return manager.get_status()

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§  í†µí•© ì–¸ì–´ëª¨ë¸ ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    manager = UnifiedLanguageModelManager()
    
    # ìƒíƒœ ì¶œë ¥
    status = manager.get_status()
    print(f"ğŸ“Š í˜„ì¬ ìƒíƒœ: {json.dumps(status, indent=2, ensure_ascii=False)}")
    
    # ê°„ë‹¨í•œ ëŒ€í™” í…ŒìŠ¤íŠ¸
    if manager.current_llm:
        print("\nğŸ’¬ ëŒ€í™” í…ŒìŠ¤íŠ¸:")
        response = manager.generate_response(
            "ì•ˆë…•! í•¨ê»˜ ë˜ì „ì„ íƒí—˜í•´ë³´ì!",
            character_name="ë¡œë°”íŠ¸",
            character_class="ì „ì‚¬",
            personality="ìš©ê°í•œ",
            situation="ë˜ì „ ì…êµ¬"
        )
        print(f"AI: {response}")
    else:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì–¸ì–´ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
