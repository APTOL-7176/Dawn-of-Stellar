"""
ğŸ¤– Dawn of Stellar - ì‹¤ì œ ì–¸ì–´ëª¨ë¸ ì—°ë™ ì‹œìŠ¤í…œ
OpenAI GPT, Claude, Ollama ë“± ì‹¤ì œ LLMê³¼ ì—°ë™

2025ë…„ 8ì›” 10ì¼ - ì‹¤ì œ AIì™€ ëŒ€í™”í•˜ëŠ” ë¡œë°”íŠ¸ë“¤!
"""

import os
import json
import asyncio
import aiohttp
import requests
from datetime import datetime
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum

class LLMProvider(Enum):
    """ì§€ì›í•˜ëŠ” ì–¸ì–´ëª¨ë¸ ì œê³µì"""
    OPENAI = "openai"
    CLAUDE = "claude"
    OLLAMA = "ollama"
    HUGGINGFACE = "huggingface"
    GEMINI = "gemini"
    LOCAL = "local"

@dataclass
class LLMConfig:
    """ì–¸ì–´ëª¨ë¸ ì„¤ì •"""
    provider: LLMProvider
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    model_name: str = "gpt-3.5-turbo"
    max_tokens: int = 150
    temperature: float = 0.7
    enabled: bool = False

class RealLanguageModelSystem:
    """ì‹¤ì œ ì–¸ì–´ëª¨ë¸ ì—°ë™ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.configs: Dict[LLMProvider, LLMConfig] = {}
        self.active_provider: Optional[LLMProvider] = None
        self.conversation_history: List[Dict[str, str]] = []
        
        # ì„¤ì • ë¡œë“œ
        self._load_configs()
        
        print("ğŸ”— ì‹¤ì œ ì–¸ì–´ëª¨ë¸ ì—°ë™ ì‹œìŠ¤í…œ ì´ˆê¸°í™”!")
        self._check_available_providers()
    
    def _load_configs(self):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        config_file = "llm_config.json"
        
        if os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                for provider_name, config_data in data.items():
                    try:
                        provider = LLMProvider(provider_name)
                        self.configs[provider] = LLMConfig(
                            provider=provider,
                            api_key=config_data.get('api_key'),
                            base_url=config_data.get('base_url'),
                            model_name=config_data.get('model_name', 'gpt-3.5-turbo'),
                            max_tokens=config_data.get('max_tokens', 150),
                            temperature=config_data.get('temperature', 0.7),
                            enabled=config_data.get('enabled', False)
                        )
                    except ValueError:
                        continue
            except Exception as e:
                print(f"âš ï¸ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ê¸°ë³¸ ì„¤ì • ìƒì„±
        self._create_default_configs()
    
    def _create_default_configs(self):
        """ê¸°ë³¸ ì„¤ì • ìƒì„±"""
        default_configs = {
            LLMProvider.OPENAI: LLMConfig(
                provider=LLMProvider.OPENAI,
                model_name="gpt-5",  # ğŸ”¥ GPT-5 ê¸°ë³¸ê°’ìœ¼ë¡œ!
                max_tokens=150,
                temperature=0.7
            ),
            LLMProvider.CLAUDE: LLMConfig(
                provider=LLMProvider.CLAUDE,
                model_name="claude-3-haiku-20240307",
                max_tokens=150,
                temperature=0.7
            ),
            LLMProvider.OLLAMA: LLMConfig(
                provider=LLMProvider.OLLAMA,
                base_url="http://localhost:11434",
                model_name="exaone3.5:7.8b",  # ğŸ‡°ğŸ‡· í™•ì‹¤íˆ ì¡´ì¬í•˜ëŠ” LG AI í•œêµ­ì–´ ëª¨ë¸!
                max_tokens=150,
                temperature=0.7
            ),
            LLMProvider.GEMINI: LLMConfig(
                provider=LLMProvider.GEMINI,
                model_name="gemini-pro",
                max_tokens=150,
                temperature=0.7
            )
        }
        
        for provider, config in default_configs.items():
            if provider not in self.configs:
                self.configs[provider] = config
    
    def _check_available_providers(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì œê³µì í™•ì¸"""
        print("\nğŸ” ì–¸ì–´ëª¨ë¸ ì œê³µì í™•ì¸:")
        
        # OpenAI API í‚¤ í™•ì¸
        openai_key = os.getenv('OPENAI_API_KEY') or self.configs.get(LLMProvider.OPENAI, LLMConfig(LLMProvider.OPENAI)).api_key
        if openai_key:
            self.configs[LLMProvider.OPENAI].api_key = openai_key
            self.configs[LLMProvider.OPENAI].enabled = True
            print("  âœ… OpenAI GPT: ì‚¬ìš© ê°€ëŠ¥")
            if not self.active_provider:
                self.active_provider = LLMProvider.OPENAI
        else:
            print("  âš ï¸ OpenAI GPT: API í‚¤ í•„ìš”")
        
        # Claude API í‚¤ í™•ì¸
        claude_key = os.getenv('ANTHROPIC_API_KEY') or self.configs.get(LLMProvider.CLAUDE, LLMConfig(LLMProvider.CLAUDE)).api_key
        if claude_key:
            self.configs[LLMProvider.CLAUDE].api_key = claude_key
            self.configs[LLMProvider.CLAUDE].enabled = True
            print("  âœ… Claude: ì‚¬ìš© ê°€ëŠ¥")
            if not self.active_provider:
                self.active_provider = LLMProvider.CLAUDE
        else:
            print("  âš ï¸ Claude: API í‚¤ í•„ìš”")
        
        # Gemini API í‚¤ í™•ì¸
        gemini_key = os.getenv('GOOGLE_API_KEY') or self.configs.get(LLMProvider.GEMINI, LLMConfig(LLMProvider.GEMINI)).api_key
        if gemini_key:
            self.configs[LLMProvider.GEMINI].api_key = gemini_key
            self.configs[LLMProvider.GEMINI].enabled = True
            print("  âœ… Gemini: ì‚¬ìš© ê°€ëŠ¥")
            if not self.active_provider:
                self.active_provider = LLMProvider.GEMINI
        else:
            print("  âš ï¸ Gemini: API í‚¤ í•„ìš”")
        
        # Ollama ë¡œì»¬ ì„œë²„ í™•ì¸
        if self._check_ollama_server():
            self.configs[LLMProvider.OLLAMA].enabled = True
            print("  âœ… Ollama: ë¡œì»¬ ì„œë²„ í™œì„±í™”")
            if not self.active_provider:
                self.active_provider = LLMProvider.OLLAMA
        else:
            print("  âš ï¸ Ollama: ë¡œì»¬ ì„œë²„ ë¯¸ì‹¤í–‰")
        
        if self.active_provider:
            print(f"\nğŸ¯ í™œì„± ì œê³µì: {self.active_provider.value}")
        else:
            print("\nâŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì–¸ì–´ëª¨ë¸ ì—†ìŒ")
            print("ğŸ’¡ API í‚¤ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ Ollamaë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”!")
    
    def _check_ollama_server(self) -> bool:
        """Ollama ì„œë²„ í™•ì¸"""
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    async def generate_response(self, prompt: str, character_context: str = "", job_class: str = "ë¡œë°”íŠ¸") -> str:
        """ì‹¤ì œ ì–¸ì–´ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±"""
        if not self.active_provider:
            return self._fallback_response(prompt, job_class)
        
        try:
            config = self.configs[self.active_provider]
            
            # ìºë¦­í„° ë§¥ë½ í¬í•¨ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            full_prompt = self._build_character_prompt(prompt, character_context, job_class)
            
            if self.active_provider == LLMProvider.OPENAI:
                return await self._call_openai(full_prompt, config)
            elif self.active_provider == LLMProvider.CLAUDE:
                return await self._call_claude(full_prompt, config)
            elif self.active_provider == LLMProvider.OLLAMA:
                return await self._call_ollama(full_prompt, config)
            elif self.active_provider == LLMProvider.GEMINI:
                return await self._call_gemini(full_prompt, config)
            else:
                return self._fallback_response(prompt, job_class)
                
        except Exception as e:
            print(f"âš ï¸ ì–¸ì–´ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return self._fallback_response(prompt, job_class)
    
    def _build_character_prompt(self, prompt: str, context: str, job_class: str) -> str:
        """ìºë¦­í„° ë§¥ë½ì´ í¬í•¨ëœ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        character_prompt = f"""ë‹¹ì‹ ì€ Dawn of Stellar ê²Œì„ì˜ {job_class} ì§ì—… AI ìºë¦­í„°ì…ë‹ˆë‹¤.

ìºë¦­í„° ì„¤ì •:
- ì§ì—…: {job_class}
- ì„±ê²©: ìë‘ìŠ¤ëŸ½ê³  ì¥ë‚œê¸° ìˆëŠ” ë¡œë°”íŠ¸
- ë§íˆ¬: ì¹œê·¼í•˜ê³  ìœ ë¨¸ëŸ¬ìŠ¤, ë•Œë¡œëŠ” ê±´ë°©ì§„ í†¤
- íŠ¹ì§•: ê²Œì„ì— ëŒ€í•œ ê¹Šì€ ì§€ì‹ê³¼ ìì‹ ê°

{context}

í”Œë ˆì´ì–´ì˜ ë§: "{prompt}"

ìœ„ ì„¤ì •ì— ë§ê²Œ {job_class} ë¡œë°”íŠ¸ë¡œì„œ ì‘ë‹µí•´ì£¼ì„¸ìš”. 
ì‘ë‹µì€ 100ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ, ìºë¦­í„°ì˜ ê°œì„±ì´ ë“œëŸ¬ë‚˜ë„ë¡ ì‘ì„±í•˜ì„¸ìš”.

ì‘ë‹µ:"""
        
        return character_prompt
    
    async def _call_openai(self, prompt: str, config: LLMConfig) -> str:
        """OpenAI API í˜¸ì¶œ"""
        headers = {
            "Authorization": f"Bearer {config.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": config.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": config.max_tokens,
            "temperature": config.temperature
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    return result["choices"][0]["message"]["content"].strip()
                else:
                    raise Exception(f"OpenAI API ì˜¤ë¥˜: {response.status}")
    
    async def _call_claude(self, prompt: str, config: LLMConfig) -> str:
        """Claude API í˜¸ì¶œ"""
        headers = {
            "x-api-key": config.api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
        data = {
            "model": config.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": config.max_tokens,
            "temperature": config.temperature
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                "https://api.anthropic.com/v1/messages",
                headers=headers,
                json=data,
                timeout=30
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    return result["content"][0]["text"].strip()
                else:
                    raise Exception(f"Claude API ì˜¤ë¥˜: {response.status}")
    
    async def _call_ollama(self, prompt: str, config: LLMConfig) -> str:
        """Ollama API í˜¸ì¶œ"""
        data = {
            "model": config.model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": config.temperature,
                "num_predict": config.max_tokens
            }
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{config.base_url}/api/generate",
                json=data,
                timeout=60
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    return result["response"].strip()
                else:
                    raise Exception(f"Ollama API ì˜¤ë¥˜: {response.status}")
    
    async def _call_gemini(self, prompt: str, config: LLMConfig) -> str:
        """Gemini API í˜¸ì¶œ"""
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{config.model_name}:generateContent?key={config.api_key}"
        
        data = {
            "contents": [{
                "parts": [{"text": prompt}]
            }],
            "generationConfig": {
                "temperature": config.temperature,
                "maxOutputTokens": config.max_tokens
            }
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=data, timeout=30) as response:
                if response.status == 200:
                    result = await response.json()
                    return result["candidates"][0]["content"]["parts"][0]["text"].strip()
                else:
                    raise Exception(f"Gemini API ì˜¤ë¥˜: {response.status}")
    
    def _fallback_response(self, prompt: str, job_class: str) -> str:
        """ëŒ€ì²´ ì‘ë‹µ (API ì‹¤íŒ¨ ì‹œ)"""
        fallback_responses = [
            f"í ... {job_class}ì¸ ë‚´ê°€ ê·¸ëŸ° ê±´ ì˜ ëª¨ë¥´ê² ë„¤! ğŸ¤”",
            f"ì˜¤ëŠ˜ì€ ë§ì´ ì•ˆ ë‚˜ì˜¤ëŠ”êµ¬ë§Œ! {job_class}ë‹µì§€ ì•Šê²Œ... ğŸ˜…",
            f"ì´ëŸ°, ë„¤íŠ¸ì›Œí¬ê°€ ë¬¸ì œì¸ê°€? {job_class}ë„ ê°€ë”ì€ ë§ë¬¸ì´ ë§‰íˆì§€! ğŸŒ",
            f"{job_class}ì¸ ë‚˜ë„ ê°€ë”ì€ ìƒê°í•  ì‹œê°„ì´ í•„ìš”í•´! â°",
            f"ì–¸ì–´ëª¨ë¸ ì—°ê²°ì´ ë¶ˆì•ˆì •í•˜ë„¤... {job_class}ì˜ í˜ìœ¼ë¡œë„ ì–´ì©” ìˆ˜ ì—†ì–´! ğŸ”Œ"
        ]
        
        import random
        return random.choice(fallback_responses)
    
    def save_config(self):
        """ì„¤ì • ì €ì¥"""
        config_data = {}
        
        for provider, config in self.configs.items():
            config_data[provider.value] = {
                "api_key": config.api_key,
                "base_url": config.base_url,
                "model_name": config.model_name,
                "max_tokens": config.max_tokens,
                "temperature": config.temperature,
                "enabled": config.enabled
            }
        
        with open("llm_config.json", 'w', encoding='utf-8') as f:
            json.dump(config_data, f, indent=2, ensure_ascii=False)
        
        print("ğŸ’¾ ì–¸ì–´ëª¨ë¸ ì„¤ì • ì €ì¥ ì™„ë£Œ!")
    
    def set_api_key(self, provider: LLMProvider, api_key: str):
        """API í‚¤ ì„¤ì •"""
        if provider in self.configs:
            self.configs[provider].api_key = api_key
            self.configs[provider].enabled = True
            
            if not self.active_provider:
                self.active_provider = provider
            
            print(f"ğŸ”‘ {provider.value} API í‚¤ ì„¤ì • ì™„ë£Œ!")
        else:
            print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì œê³µì: {provider.value}")
    
    def switch_provider(self, provider: LLMProvider):
        """ì–¸ì–´ëª¨ë¸ ì œê³µì ë³€ê²½"""
        if provider in self.configs and self.configs[provider].enabled:
            self.active_provider = provider
            print(f"ğŸ”„ ì–¸ì–´ëª¨ë¸ ë³€ê²½: {provider.value}")
        else:
            print(f"âŒ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ì œê³µì: {provider.value}")
    
    def get_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ë°˜í™˜"""
        status = {
            "active_provider": self.active_provider.value if self.active_provider else None,
            "available_providers": [],
            "total_conversations": len(self.conversation_history)
        }
        
        for provider, config in self.configs.items():
            if config.enabled:
                status["available_providers"].append({
                    "name": provider.value,
                    "model": config.model_name,
                    "active": provider == self.active_provider
                })
        
        return status

class InteractiveRobatChat:
    """ìƒí˜¸ì‘ìš©í•˜ëŠ” ë¡œë°”íŠ¸ ì±„íŒ… ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.llm_system = RealLanguageModelSystem()
        self.active_character = "ì „ì‚¬"
        self.character_contexts = self._load_character_contexts()
        
        print("\nğŸ’¬ ìƒí˜¸ì‘ìš©í•˜ëŠ” ë¡œë°”íŠ¸ ì±„íŒ… ì‹œìŠ¤í…œ ì‹œì‘!")
    
    def _load_character_contexts(self) -> Dict[str, str]:
        """ìºë¦­í„°ë³„ ë§¥ë½ ë¡œë“œ"""
        contexts = {
            "ì „ì‚¬": "ë‹¹ì‹ ì€ ìš©ê°í•˜ê³  ê°•ì¸í•œ ì „ì‚¬ì…ë‹ˆë‹¤. ì „íˆ¬ì— ëŒ€í•œ ìì‹ ê°ì´ ë„˜ì¹˜ê³ , ì •ì˜ê°ì´ ê°•í•©ë‹ˆë‹¤.",
            "ì•„í¬ë©”ì´ì§€": "ë‹¹ì‹ ì€ ì§€í˜œë¡­ê³  ì‹ ë¹„ë¡œìš´ ëŒ€ë§ˆë²•ì‚¬ì…ë‹ˆë‹¤. ë§ˆë²•ì— ëŒ€í•œ ê¹Šì€ ì§€ì‹ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.",
            "ê¶ìˆ˜": "ë‹¹ì‹ ì€ ì •í™•í•˜ê³  ì¹¨ì°©í•œ ê¶ìˆ˜ì…ë‹ˆë‹¤. ì›ê±°ë¦¬ ì „íˆ¬ì˜ ë‹¬ì¸ì´ë©° ê´€ì°°ë ¥ì´ ë›°ì–´ë‚©ë‹ˆë‹¤.",
            "ë„ì ": "ë‹¹ì‹ ì€ ì˜ë¦¬í•˜ê³  ì¬ë¹ ë¥¸ ë„ì ì…ë‹ˆë‹¤. ì€ë°€í•œ í–‰ë™ê³¼ ê¸°íšŒì£¼ì˜ì  ì„±ê²©ì„ ê°€ì¡ŒìŠµë‹ˆë‹¤.",
            "ì„±ê¸°ì‚¬": "ë‹¹ì‹ ì€ ê³ ê²°í•˜ê³  ì‹ ì„±í•œ ê¸°ì‚¬ì…ë‹ˆë‹¤. ì •ì˜ì™€ ì‹ ì•™ì‹¬ì´ ê¹Šìœ¼ë©° ë™ë£Œë¥¼ ë³´í˜¸í•©ë‹ˆë‹¤.",
            "ë¡œë°”íŠ¸": "ë‹¹ì‹ ì€ ê²Œì„ì˜ ë§ˆìŠ¤ì½”íŠ¸ ë¡œë°”íŠ¸ì…ë‹ˆë‹¤. ìë‘ìŠ¤ëŸ½ê³  ì¥ë‚œê¸° ìˆìœ¼ë©° ê²Œì„ì„ ì‚¬ë‘í•©ë‹ˆë‹¤."
        }
        
        return contexts
    
    async def chat_with_robat(self, message: str, character: str = None) -> str:
        """ë¡œë°”íŠ¸ì™€ ì±„íŒ…"""
        if character:
            self.active_character = character
        
        context = self.character_contexts.get(self.active_character, self.character_contexts["ë¡œë°”íŠ¸"])
        
        response = await self.llm_system.generate_response(
            message, 
            context, 
            self.active_character
        )
        
        # ëŒ€í™” ê¸°ë¡ ì €ì¥
        self.llm_system.conversation_history.append({
            "timestamp": datetime.now().isoformat(),
            "character": self.active_character,
            "user_message": message,
            "robat_response": response
        })
        
        return response
    
    def run_interactive_session(self):
        """ëŒ€í™”í˜• ì„¸ì…˜ ì‹¤í–‰"""
        print(f"\nğŸ¤– {self.active_character} ë¡œë°”íŠ¸ì™€ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤!")
        print("(ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥)")
        print("-" * 50)
        
        while True:
            try:
                user_input = input(f"\nğŸ’¬ ë‹¹ì‹ : ")
                
                if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                    print(f"\nğŸ‘‹ {self.active_character} ë¡œë°”íŠ¸: ì•ˆë…•! ë˜ ë§Œë‚˜ì!")
                    break
                
                if user_input.lower().startswith('switch '):
                    new_char = user_input[7:].strip()
                    if new_char in self.character_contexts:
                        self.active_character = new_char
                        print(f"ğŸ”„ {new_char} ë¡œë°”íŠ¸ë¡œ ë³€ê²½!")
                        continue
                
                print(f"ğŸ¤– {self.active_character} ë¡œë°”íŠ¸ê°€ ìƒê° ì¤‘...")
                
                response = asyncio.run(self.chat_with_robat(user_input))
                print(f"ğŸ¤– {self.active_character} ë¡œë°”íŠ¸: {response}")
                
            except KeyboardInterrupt:
                print(f"\nğŸ‘‹ {self.active_character} ë¡œë°”íŠ¸: ê°‘ìê¸° ì™œ ê·¸ë˜? ì•„ë¬´íŠ¼ ì•ˆë…•!")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

def setup_api_keys():
    """API í‚¤ ì„¤ì • ë„ìš°ë¯¸"""
    print("ğŸ”‘ ì–¸ì–´ëª¨ë¸ API í‚¤ ì„¤ì •")
    print("-" * 30)
    
    llm_system = RealLanguageModelSystem()
    
    print("ì‚¬ìš© ê°€ëŠ¥í•œ ì–¸ì–´ëª¨ë¸:")
    print("1. OpenAI GPT (ChatGPT)")
    print("2. Claude (Anthropic)")
    print("3. Gemini (Google)")
    print("4. Ollama (ë¡œì»¬)")
    print()
    
    choice = input("ì„¤ì •í•  ì–¸ì–´ëª¨ë¸ ë²ˆí˜¸ (1-4): ").strip()
    
    if choice == "1":
        api_key = input("OpenAI API í‚¤ ì…ë ¥: ").strip()
        if api_key:
            llm_system.set_api_key(LLMProvider.OPENAI, api_key)
    elif choice == "2":
        api_key = input("Anthropic API í‚¤ ì…ë ¥: ").strip()
        if api_key:
            llm_system.set_api_key(LLMProvider.CLAUDE, api_key)
    elif choice == "3":
        api_key = input("Google API í‚¤ ì…ë ¥: ").strip()
        if api_key:
            llm_system.set_api_key(LLMProvider.GEMINI, api_key)
    elif choice == "4":
        print("Ollama ì„¤ì¹˜ ê°€ì´ë“œ:")
        print("1. https://ollama.ai ì—ì„œ ë‹¤ìš´ë¡œë“œ")
        print("2. ì„¤ì¹˜ í›„ 'ollama pull llama2' ì‹¤í–‰")
        print("3. 'ollama serve' ë¡œ ì„œë²„ ì‹œì‘")
    
    llm_system.save_config()

async def demo_real_llm_integration():
    """ì‹¤ì œ ì–¸ì–´ëª¨ë¸ ì—°ë™ ë°ëª¨"""
    print("ğŸ® === Dawn of Stellar ì‹¤ì œ ì–¸ì–´ëª¨ë¸ ì—°ë™ ë°ëª¨! ===")
    print()
    
    # ì–¸ì–´ëª¨ë¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    llm_system = RealLanguageModelSystem()
    
    if not llm_system.active_provider:
        print("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ì–¸ì–´ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
        print("API í‚¤ë¥¼ ì„¤ì •í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ", end="")
        if input().lower().startswith('y'):
            setup_api_keys()
            return
        else:
            print("ğŸ’¡ ë‚˜ì¤‘ì— API í‚¤ë¥¼ ì„¤ì •í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”!")
            return
    
    # ìƒíƒœ ì¶œë ¥
    status = llm_system.get_status()
    print(f"ğŸ¯ í™œì„± ëª¨ë¸: {status['active_provider']}")
    print(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ìˆ˜: {len(status['available_providers'])}")
    print()
    
    # ëŒ€í™”í˜• ì„¸ì…˜
    chat_system = InteractiveRobatChat()
    chat_system.run_interactive_session()

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "setup":
            setup_api_keys()
        elif sys.argv[1] == "chat":
            asyncio.run(demo_real_llm_integration())
        else:
            print("ì‚¬ìš©ë²•: python ai_language_model_integration.py [setup|chat]")
    else:
        print("ğŸ¤– Dawn of Stellar ì–¸ì–´ëª¨ë¸ ì—°ë™ ì‹œìŠ¤í…œ")
        print("ì‚¬ìš©ë²•:")
        print("  python ai_language_model_integration.py setup  # API í‚¤ ì„¤ì •")
        print("  python ai_language_model_integration.py chat   # ë¡œë°”íŠ¸ì™€ ëŒ€í™”")
